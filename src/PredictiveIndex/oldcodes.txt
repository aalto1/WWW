

public void processWARCRecord(String[] words, String title) throws IOException {
	
        /*This function process the single wrac files.
	REMOVED: in this version i do not save the information of the single doc, in this way i am obliged to scan the collection
	3 times without any necessity. I have done two major improvement between which the second is better because has less overhead
	using hashmap stored in a 1d array with the double size of the doc termID1, freqtermID. If there is no entry it means that
	that word has just 1 has frequency */

        this.docsMap.putIfAbsent(title, this.stats[0]);
        HashMap<String, Integer> auxHash = new HashMap<>();
        String word;
        int [] intWords = new int[words.length+1];
        intWords[0] = this.stats[0];

        for (int k = 0; k<words.length-1; k++) {
            //words.length-1 because the last item of the array is the title
            word = words[k];
            if (auxHash.putIfAbsent(word, 1) == null) {
                if (!this.termsMap.containsKey(word)) {
                    this.termsMap.put(word, stats[2]);
                    this.freqTermDoc.put(stats[2], 1);
                    this.stats[2]++; //unique words
                } else {
                    this.freqTermDoc.merge(getWId(word), 1, Integer::sum);
                }
            }
            intWords[k+1] = getWId(word);
        }
        this.forwardIndexFile.writeObject(intWords);
        //this.forwardIndexFile.flush();
        this.stats[0]++;
        this.stats[1] += words.length;
    }

public void processWARCRecord2(String[] words, String title) throws IOException {
        /*this function process the single wrac files. In this version I was storing an 1d array in which the position was 
	added to an hashmap storing the term position. So I check if the term is in the hashmap if not it means that is the
	first time that we meet this term and its freq is the next position in the freq array. We get this info and add a new
	entry to the hashmap*/

        int docID = this.stats[0];
        this.docsMap.putIfAbsent(title, docID);
        int [] intWords = new int[words.length+1];
        intWords[0] = this.stats[0];                                        //frist element is the doc title
        HashMap<Integer, Integer> position = new HashMap<>();
        ArrayList<Integer> singeDocStats = new ArrayList<>();
        int pos;
        int uniqueWords = 0;

        int termID;
        for (int k = 0; k<words.length-1; k++) {
            termID = putWordIfAbstent(words[k]);
            intWords[k+1] = termID;
            //We use and auxiliary array to store the frequency whitin every word inside the document

            if (position.putIfAbsent(termID, uniqueWords) == null){
                singeDocStats.add(1);
                uniqueWords++;
                this.freqTermDoc.merge(termID, 1, Integer::sum);
            }else{
                pos = position.get(termID);
                singeDocStats.set(pos, singeDocStats.get(pos)+1);
            }
        }
        int [] aux =  singeDocStats.stream().mapToInt(Integer::valueOf).toArray();
        this.forwardIndexFile.writeObject(intWords);
        this.forwardIndexFile.writeObject(aux);
        //this.forwardIndexFile.flush();
        this.stats[0]++;
        this.stats[1] += words.length;
    }

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

/* We use a temporaney HashMap to process the single documents present in the WRAC document and the we flush it
        and then we flush it in the buffer. The important thing is to do not add duplicate and do not get self pairs.
        THE FIRST ELEMENT IS THE TITLE
	
	int uniqueWords = 0;
        //f1 = docTermsStats[getTermPos(pair[0], uniqueWords, position)];
        //f2 = docTermsStats[getTermPos(pair[1], uniqueWords, position)];
        
	/*HashMap<Integer, Integer> docFreq = new HashMap<>();    //auxFreq counts for each term how many times is used into the document
        for (int wIx = 1; wIx < words.length; wIx++) {
            if (docFreq.putIfAbsent(words[wIx], 1) != null) {
                docFreq.merge(words[wIx], 1, Integer::sum);    //modified from this.freqTermDoc errore
            }
        }*/

public static int getTermPos(int termID, int uniqueWords, HashMap<Integer,Integer> position){
	//REMOVED: because the array containg the ones would take about 20% more than the hash-1darray approach

        if (position.putIfAbsent(termID, uniqueWords) == null){
            uniqueWords++;
            return (uniqueWords - 1);
        }else{
            return position.get(termID);
        }
    }

//int [] position = (int[]) ois.readObject();
//this.bufferedIndex(intWords, intWords[0], position);

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

//*/
	// ARRAY TO DO TESTS
     /*InvertedIndex a = new InvertedIndex();
     int [][] aux = new int[][] {
                {3,4,3232,2,1},
                {1,5,3234,1,1},
                {3,6,7532, 4,1},
                {2,10, 53421, 5,1},
                {2,11,4141,1,1},
                {2,12,541,1,1},
                {1,2,41415,1,2},
                {3,1, 5145,1,2},
                {3,2,311,7,1},
                {2,9,41113,5,1},
                {1,3,2114,5,1},
                {3,4,3212,131,1},
                {1,5,412,4141,1},
                {3,6,4141,4141,1},
                {2,1,414,4141,1},
                {2,1,434,3,1},
                {2,12,22,3,1},
                {1,1,2,4,1, 34},
                {1,2,2131,4,1},
                {3,7,4,43,1},
                {3,8,4,4,1},
                {2,9,44,31,1},
                {1,3,31,31,1},
                {1,2,434,4343,1},
                {1,2,31,313,1},
                {3,7,31231,3231,1},
                {3,8,333,555,1},
                {2,9,5555,41,1},
                {2,44,41,4141,1}
        };
        a.buffer= aux;
        a.naturalSelection();*/



